{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRN Normalization\n",
    "\n",
    "1.  Loading data into a gene expression matrix.\n",
    "2.  Munging data\n",
    "3.  MRN Normalization\n",
    "4.  sanity checks between clones \n",
    "\n",
    "\"Trimmed Mean of M-values\" (TMM) normalization, published by [Robinson and Oshlack](https://www.frontiersin.org/articles/10.3389/fgene.2016.00164/full#B16) is a widely used method of normalizing gene expression in scRNA data.  A variant called MRN (Median Ratio Normalization) is described by [Maza et al.](https://www.tandfonline.com/doi/full/10.4161/cib.25849), and may perform slightly better than TMM.  We carry out MRN normalization on the P9855 RNA data, and use the results in a quick machine learning application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages.  Put plots \"inline\" in the notebook.  \n",
    "\n",
    "import numpy as np  # For numerical computations.\n",
    "import pandas as pd  # Pandas for data analysis.\n",
    "import matplotlib.pyplot as plt  # For basic plotting.\n",
    "import seaborn as sns # For pretty visualization in Seaborn.  See https://seaborn.pydata.org/\n",
    "\n",
    "import os # Working with file directories, etc.\n",
    "\n",
    "from IPython.display import display # Pretty display of data frames.\n",
    "\n",
    "# Put plots inline rather than in a pop-up.\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #got annoyed of these red boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to section 2, if you have already loaded the data and have the pickles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Loading the data\n",
    "\n",
    "Start at section 2, if the raw data has already been processed and pickled.\n",
    "\n",
    "In this section, we load the gene expression data and some metadata.  The sequencing data is stored in a 'merged_gene_counts.txt'.  The metadata is stored in a separate file called 'meta_data_marty_inVitro_feb8.csv'.  We use the metadata to select bulk-cells from  experiment P9855.  The first function loads the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(experiment, filename = 'meta_data_marty_inVitro_feb8.csv', report=True, bulks = False):\n",
    "    df = pd.read_csv(filename, sep=',',\n",
    "                     index_col=2, header=0, low_memory=False)\n",
    "    df = df[ df['Project_ID'] == experiment] # Only cells from the experiment.\n",
    "    if bulks:\n",
    "        df = df[ df['Number_Of_Cells'] >= 25] # Include only bulks.\n",
    "    else:\n",
    "        df = df[ df['Number_Of_Cells'] == 1] # Get rid of bulks.\n",
    "    if report:\n",
    "        if bulks:\n",
    "            print('{} bulks found in experiment {}'.format(len(df), experiment))\n",
    "        else:\n",
    "            print('{} single cells found in experiment {}'.format(len(df), experiment))\n",
    "        clones = df['Clone_ID'].unique()\n",
    "        print('{} Clones: {}'.format(len(clones), ', '.join(clones)))\n",
    "        print('The first five rows of the dataframe are below')\n",
    "        display(df[:5])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_df = get_meta('P9855', filename = 'meta_data_marty_inVitro_feb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_bulks = get_meta('P9855', filename = 'meta_data_marty_inVitro_feb8.csv', bulks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_bulks.drop(meta_bulks.index[[range(69,99)]], inplace=True) # drop atac data... and c12 while we are cutting\n",
    "                                                                # the tail off of the dataframe \n",
    "meta_bulks.drop(['In_Vivo_Clone_ID'],axis=1, inplace=True)\n",
    "meta_bulks.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_bulks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_bulks['Clone_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequencing data (in merged_gene_counts.txt) contains a separate row for each  gene.  The name of each column contains list of filenames (I'm guessing input for star), we will clean up column names.\n",
    "But first, to take out bulks that are negatives or otherwise messed up... I'm doing this the brute way by excluding by a list. *If I were clever I would look at rows as triplets and make sure all three have 25 cells.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_samples = ['P9855_2016', 'P9855_2017', 'P9855_2018', 'P9855_2043', 'P9855_2044', 'P9855_2045', 'P9855_2046', 'P9855_2047', 'P9855_2048', 'P9855_2055', 'P9855_2056', 'P9855_2057', 'P9855_2082', 'P9855_2083', 'P9855_2084']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "read_merged_genes = pd.read_csv('merged_gene_counts.txt', sep='\\t', index_col=0)\n",
    "read_merged_genes.columns = read_merged_genes.columns.str[:10] # 10 characters in P9855_20**\n",
    "read_merged_genes.drop(['gene_name'],axis=1, inplace=True)\n",
    "read_merged_genes = read_merged_genes.transpose() # tanspose dataframe for later analysis infrastructure\n",
    "read_merged_genes.drop(list(read_merged_genes.index & exclude_samples), inplace=True)\n",
    "read_merged_genes.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "read_merged_genes.shape # great, the sizes for the meta and the rawreads have equal index sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_merged_genes.to_pickle('P9855_rawreads.pkl') # Save file as a pickle.\n",
    "meta_bulks.to_pickle('P9855_meta.pkl') # Pickle the metadata too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function loads an entire *list* of cells, and places their gene expression data into the rows of a matrix.  The rows are indexed by the cell names, and the columns by genes.  The data is the gene expression, as raw number of reads.  This may take a little while, so we give progress updates every 10 cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Munging data\n",
    "\n",
    "Start here if you have the pickles!  We filter the data a bit, before normalization downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulks_raw = pd.read_pickle('P9855_rawreads.pkl') # Load bulks expression matrix from a pickle.\n",
    "meta_df = pd.read_pickle('P9855_meta.pkl') # Load metadata from a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = list(bulks_raw.columns)  # The names of the genes. \n",
    "bulks = list(bulks_raw.index) # The names of the bulks.\n",
    "clones = sorted(list(meta_df.Clone_ID.unique())) # The names of the clones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing TCRs and rarely-expressed genes\n",
    "\n",
    "T cells have special genetically rearranged receptors called TCRs.  These are made of segments called TRBV9, TRBJ2-4, TRAV12-2, TRAJ14, etc.  Bascally any gene that is called these letters followed by a number -- TRBV, TRBJ, TRAV, TRAJ -- is part of this receptor and they are defined as being clonal.  Therefore we exclude these genes since we want to find more interesting similarities within clonal populations.\n",
    "\n",
    "The following loads a list of genes to be excluded from the data for later analysis.  The excluded genes should be given in a csv file with *one* column.  No row labels should be given.  The first row should be a descriptive header, like \"Genes to exclude.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exc_filename = 'TRgenes.csv'  # CHANGE this if needed.  I added TRAC and TRDV3 as requested.\n",
    "exc_df = pd.read_csv(exc_filename, sep=',', header=0)\n",
    "exclude_genes = exc_df.iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we switch the labels from the hgnc name to the ensemble ID because there were two instances with the same hgnc name, however, they have unique ensemble IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_hgnc_meta_filename = 'All_Gene_Info_Metadata.csv'\n",
    "ensembl_hgnc_meta_df = pd.read_csv(ensembl_hgnc_meta_filename, sep=',', header=0)\n",
    "ensembl_genes = pd.Series(ensembl_hgnc_meta_df.ensembl_gene_id.values,index=ensembl_hgnc_meta_df.hgnc_symbol).to_dict()\n",
    "exclude_ensembl = [ensemblID for hgnc,ensemblID in ensembl_genes.items() if hgnc in exclude_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant(gf, eg, prevalence=0.05, threshold = 10): # gf = expression matrix, eg = list of genes to exclude\n",
    "    '''\n",
    "    Outputs True if the gene is relevant for analysis.  We throw out excluded genes.\n",
    "    By default, we take genes that are found in at least 5% of all cells at a level of\n",
    "    10 counts or more.\n",
    "    '''\n",
    "    nonzero_count = (gf > threshold).sum(axis=0)\n",
    "    nonzero_proportion = nonzero_count / len(gf)\n",
    "    return [gene for gene in gf.columns if\n",
    "           (gene not in eg) & \n",
    "           (nonzero_proportion[gene] > prevalence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "genes_relevant = get_relevant(bulks_raw, exclude_ensembl) # takes like a min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"{} bulks are measured, from {} to {}.\".format(len(bulks), bulks[0], bulks[-1]))\n",
    "print(\"{} genes are measured, from {} to {}.\".format(len(genes),genes[0],genes[-1]))\n",
    "genes_excluded = [gene for gene in exclude_ensembl if gene in genes]\n",
    "print(\"{} TCR genes were excluded, from {} to {}.\".format(len(genes_excluded), genes_excluded[0], genes_excluded[-1]))\n",
    "print(\"{} genes are considered relevant, from {} to {}.\".format(len(genes_relevant), genes_relevant[0], genes_relevant[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing low count libraries\n",
    "\n",
    "Some triplets (sets of clones) samples expressed too few genes. We make \"violin-plots\" giving the number of genes expressed by each cell, sorted by clonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nRead(ge, md, cutoff = 0, plot=True): # ge = gene expression, md = meta data\n",
    "    nR = ge.apply(lambda row: sum(row > cutoff), axis=1) # cutoff = number of genes expressed?\n",
    "    nR.name = 'num_reads'\n",
    "    clonalities = md.Clone_ID\n",
    "    nRead_df = pd.concat([nR, clonalities], axis=1)\n",
    "    if plot:\n",
    "        fig = plt.subplots(figsize=(18,8))\n",
    "        sns.violinplot(x=\"Clone_ID\", y=\"num_reads\", inner='quartiles', data=nRead_df)\n",
    "        sns.swarmplot(x=\"Clone_ID\", y=\"num_reads\", color=\"white\", size=3, data=nRead_df);\n",
    "    return nRead_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nR = nRead(bulks_raw, meta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop samples that have low read counts below two standard deviations from the average total raw counts across samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nR_sum = bulks_raw.apply(np.sum, axis=1) # total raw counts per sample \n",
    "nR_mean = nR_sum.mean() # across samples, the avg total raw counts\n",
    "nR_std = nR_sum.std() # across samples, the std total raw counts\n",
    "nR_lowcut = (nR_mean - 2*nR_std)\n",
    "bulks_good = [b for b in bulks if \n",
    "              (bulks_raw.sum(axis=1).loc[b] >= nR_lowcut)]\n",
    "print('{} bulks remaining after {} poor libraries removed.'.format(len(bulks_good), len(bulks) - len(bulks_good)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nR = nRead(bulks_raw.loc[bulks_good], meta_df.loc[bulks_good]) # Post-trimming violin-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore this block\n",
    "def get_bulks(ge, md, counts_threshold = 2000): # ge = gene expression, md = meta data\n",
    "    '''\n",
    "    this function adds up all the raw counts per sample, if any sample falls below the set threshold\n",
    "    the entire clone's data (the triplet of samples) will be dropped.\n",
    "    '''\n",
    "    sample_counts = ge.sort_index().sum(axis=1)\n",
    "\n",
    "    for sample in range(0, len(sample_counts),3):\n",
    "        triplet = sample_counts.iloc[sample: sample + 3].values\n",
    "        print(triplet,'a triplet')\n",
    "        #print(triplet.values,'two')\n",
    "        if np.all(triplet) > counts_threshold:\n",
    "            print('valid triplet', triplet)\n",
    "#ignore this block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3.  MRN Normalization.\n",
    "\n",
    "Here we implement MRN Normalization on the gene expression data, closely following the convenient outline in Section 3.2 of [Maza](https://www.frontiersin.org/articles/10.3389/fgene.2016.00164/full).  We begin by putting our filtered data into a dataframe.  The dataframe `EM` (for \"expression matrix\") contains the gene expressions for the cells, filtered as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EM = bulks_raw[genes_relevant].loc[bulks_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EM.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step I:  Prenormalization by library size.\n",
    "\n",
    "In Maza's article, $X_{gkr}$ stands for the raw count (number of reads) of gene $g$, for a cell number $r$ among clone $k$.  This information is contained in our expression matrix `EM`.  The first step is to normalize by library size, dividing $X_{gkr}$ by the total number of reads $N_{kr}$ of cell with numbers $k$, $r$.  \n",
    "\n",
    "We find the total number of reads for each cell by simply summing the numbers in each row of the data frame `EM`.  We don't need to worry about the separate indices $k$ and $r$ yet.  We examine the resulting \"library size\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library_size = EM.sum(axis=1) # Drop the clone column.  Sum along rows.\n",
    "print(library_size.head())\n",
    "library_size.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library size is about 1.7 million +/- 280,000.  Now we normalize the expression matrix by dividing every cell's raw counts by the cell's library size.  Note that `EM` is a dataframe whose rows are indexed by the cells.  `library_size` is a series (basically an array) whose rows are indexed by cells.  Numpy/pandas will divide one array by another, term by term, if they have the same size.  So it can divide *each column* of `EM` by `library size`, in a quickly-broadcasted division.  To perform this on every column, we use the `apply` method with a \"lambda\" function... it's the quickest method I know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = EM.apply(lambda column : column / library_size)\n",
    "Y.sort_index().head() #does not update the Y expression matrix, just for the sake of  us looking at nicely sorted rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `Y` is the dataframe with counts normalized by library size, and we pass to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step II:  Creation of reference sample.\n",
    "\n",
    "A difference between TMM (used in edgeR), RLE (used in DeSeq2), and MRN, is how they create a reference sample.  In MRN, a reference sample is created by averaging the previous dataframe `Y` over cells within a single condition (clone).  We carry this out here.\n",
    "\n",
    "Note the method-chaining in defining `Y_clonal` below.  The `assign` method tacks on a new column to `Y` for the clone_IDs.  The `groupby` method then groups cells by clonotypes.  The `apply` method then takes the means within each clonotype.  The end result is to replace the (prenormalized) gene expression for *each cell* by the averages for each clonotype.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_clonal = Y.assign(Clone=meta_df['Clone_ID']).groupby('Clone').apply(np.mean) # Takes a few seconds.\n",
    "display(Y_clonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our reference sample to be the A3 clonal average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ref = Y_clonal.loc['P1_A03'] # Our reference sample.  Basically an average of all cells of clonotype P1_A03.\n",
    "Y_ref.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step III:  Computation of size relative to reference sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute relative scaling factors for each clone, based on the median fold-changes between gene expression within that clone and the reference clone (A7, we have chosen above).  Due to dropouts (values of 0 in gene expression), we discard genes with zeros when computing fold changes.  This avoids division-by-zero problems, and it matches the implementation of [Maza et al.](https://www.tandfonline.com/doi/full/10.4161/cib.25849).  (See the R code in the supplementary information).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tau = pd.Series(index = clones)\n",
    "for clone in clones: # Why not a little loop.\n",
    "    numerator = Y_clonal.loc[clone]\n",
    "    denominator = Y_ref\n",
    "    ok_genes = [gene for gene in genes_relevant if (numerator[gene] != 0) and (denominator[gene] != 0)]\n",
    "    ratios = numerator[ok_genes] / denominator[ok_genes] # The ratios.\n",
    "    tau[clone] = np.median(ratios)\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step IV:  Adjustment of relative scaling factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step does not occur in MRN normalization.  Yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step V:  Effective library size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate series to match the clone IDs with the samples\n",
    "clonalities = pd.Series(meta_df['Clone_ID'], index=library_size.index)\n",
    "clonalities.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate series to map tau scaling values to the samples\n",
    "scalar_lib_size = clonalities.map(tau)\n",
    "scalar_lib_size.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final series, for this step at least as the library_size values are scaled by tau values\n",
    "effective_lib_size = scalar_lib_size * library_size\n",
    "effective_lib_size.sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step VI:  Computation of relative normalization/size factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_mean = np.exp(effective_lib_size.apply(np.log).mean()) # The geometric mean of the effective_lib_size series.\n",
    "geo_mean # A single number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_lib_size.mean() # Compare the geometric mean (above) to the usual mean here.  They shouldn't be *too* far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_normalization_factor = effective_lib_size / geo_mean\n",
    "relative_normalization_factor.sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step VII:  Normalization of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here we normalize our raw counts by the relative normalization factor we just\n",
    "# calculated in Step VI\n",
    "mrn_counts = EM.div(relative_normalization_factor, axis=0)\n",
    "mrn_counts.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrn_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrn_counts.to_pickle('P9855_mrn.pkl') # Pickle the counts dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrn_counts.to_csv('P9855_mrn.csv') # write the counts dataframe to a file that can be opened in excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  sanity checks between clones \n",
    "\n",
    "After preprocessing the data, one might want to quickly check how gene expression varies or lack thereof between clones.Both CD3D (ENSG00000167286) and ACTB (ENSG00000075624) just **may** be similiarly expressed between cells. Whereas, we use KLRD1 (ENSG00000134539) and IL4 (ENSG00000113520), which should both vary between clones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_pickle('P9855_mrn.pkl') # Load normalized data from a pickle.\n",
    "X_cells = X.index\n",
    "y_meta = pd.read_pickle('P9855_meta.pkl') # Load metadata from a pickle.\n",
    "y_clonal = X.assign(Clone=y_meta['Clone_ID']).groupby('Clone').apply(np.mean) # Takes a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Geneid</th>\n",
       "      <th>ENSG00000167286</th>\n",
       "      <th>ENSG00000075624</th>\n",
       "      <th>ENSG00000134539</th>\n",
       "      <th>ENSG00000113520</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clone</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P1_A03</th>\n",
       "      <td>6345.318780</td>\n",
       "      <td>4445.952739</td>\n",
       "      <td>252.247889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_A05</th>\n",
       "      <td>4919.983706</td>\n",
       "      <td>6039.982829</td>\n",
       "      <td>63.355645</td>\n",
       "      <td>60.929846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_A08</th>\n",
       "      <td>5117.387436</td>\n",
       "      <td>5021.683918</td>\n",
       "      <td>687.836586</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_A12</th>\n",
       "      <td>5301.474785</td>\n",
       "      <td>6808.072023</td>\n",
       "      <td>55.307823</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_B10</th>\n",
       "      <td>4070.314569</td>\n",
       "      <td>6079.636062</td>\n",
       "      <td>112.577324</td>\n",
       "      <td>10.695181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_C11</th>\n",
       "      <td>6884.670186</td>\n",
       "      <td>4983.576678</td>\n",
       "      <td>1006.945061</td>\n",
       "      <td>124.851286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_D09</th>\n",
       "      <td>5121.110087</td>\n",
       "      <td>13308.868565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.563975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_D12</th>\n",
       "      <td>5856.853798</td>\n",
       "      <td>4869.036502</td>\n",
       "      <td>920.274802</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_F02</th>\n",
       "      <td>4964.108172</td>\n",
       "      <td>3346.955712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_F04</th>\n",
       "      <td>6166.635529</td>\n",
       "      <td>3587.078453</td>\n",
       "      <td>529.834020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_F10</th>\n",
       "      <td>5655.304596</td>\n",
       "      <td>5661.076281</td>\n",
       "      <td>114.305688</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_H01</th>\n",
       "      <td>5495.242566</td>\n",
       "      <td>5457.914780</td>\n",
       "      <td>215.488284</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1_H10</th>\n",
       "      <td>5051.756887</td>\n",
       "      <td>7695.855074</td>\n",
       "      <td>38.135533</td>\n",
       "      <td>328.664998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_A04</th>\n",
       "      <td>5608.598977</td>\n",
       "      <td>5382.035082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_A05</th>\n",
       "      <td>5436.257275</td>\n",
       "      <td>3661.618065</td>\n",
       "      <td>196.708600</td>\n",
       "      <td>21.550527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_B11</th>\n",
       "      <td>6697.952702</td>\n",
       "      <td>4563.242899</td>\n",
       "      <td>165.232989</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_D12</th>\n",
       "      <td>5969.767662</td>\n",
       "      <td>6430.374680</td>\n",
       "      <td>190.333890</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_F01</th>\n",
       "      <td>7913.150974</td>\n",
       "      <td>5241.671313</td>\n",
       "      <td>1196.807350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_G01</th>\n",
       "      <td>6858.272060</td>\n",
       "      <td>2661.501487</td>\n",
       "      <td>118.719446</td>\n",
       "      <td>0.372379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_G09</th>\n",
       "      <td>5147.990630</td>\n",
       "      <td>3742.327501</td>\n",
       "      <td>683.228526</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_H01</th>\n",
       "      <td>6629.360736</td>\n",
       "      <td>3991.477070</td>\n",
       "      <td>75.248997</td>\n",
       "      <td>30.874441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_H04</th>\n",
       "      <td>7531.903501</td>\n",
       "      <td>8108.279492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.915451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2_H05</th>\n",
       "      <td>4715.170789</td>\n",
       "      <td>5166.287727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>258.226316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Geneid  ENSG00000167286  ENSG00000075624  ENSG00000134539  ENSG00000113520\n",
       "Clone                                                                     \n",
       "P1_A03      6345.318780      4445.952739       252.247889         0.000000\n",
       "P1_A05      4919.983706      6039.982829        63.355645        60.929846\n",
       "P1_A08      5117.387436      5021.683918       687.836586         0.000000\n",
       "P1_A12      5301.474785      6808.072023        55.307823         0.000000\n",
       "P1_B10      4070.314569      6079.636062       112.577324        10.695181\n",
       "P1_C11      6884.670186      4983.576678      1006.945061       124.851286\n",
       "P1_D09      5121.110087     13308.868565         0.000000        13.563975\n",
       "P1_D12      5856.853798      4869.036502       920.274802         0.000000\n",
       "P1_F02      4964.108172      3346.955712         0.000000         0.000000\n",
       "P1_F04      6166.635529      3587.078453       529.834020         0.000000\n",
       "P1_F10      5655.304596      5661.076281       114.305688         0.000000\n",
       "P1_H01      5495.242566      5457.914780       215.488284         0.000000\n",
       "P1_H10      5051.756887      7695.855074        38.135533       328.664998\n",
       "P2_A04      5608.598977      5382.035082         0.000000         0.000000\n",
       "P2_A05      5436.257275      3661.618065       196.708600        21.550527\n",
       "P2_B11      6697.952702      4563.242899       165.232989         0.000000\n",
       "P2_D12      5969.767662      6430.374680       190.333890         0.000000\n",
       "P2_F01      7913.150974      5241.671313      1196.807350         0.000000\n",
       "P2_G01      6858.272060      2661.501487       118.719446         0.372379\n",
       "P2_G09      5147.990630      3742.327501       683.228526         0.000000\n",
       "P2_H01      6629.360736      3991.477070        75.248997        30.874441\n",
       "P2_H04      7531.903501      8108.279492         0.000000        29.915451\n",
       "P2_H05      4715.170789      5166.287727         0.000000       258.226316"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_GOIs = y_clonal[['ENSG00000167286','ENSG00000075624','ENSG00000134539','ENSG00000113520']]\n",
    "check_GOIs #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
